
========================================
COMPREHENSIVE MODEL COMPARISON ANALYSIS
ALL MODELS: RF, XGBoost, LightGBM, CatBoost, GAM, NN, Ensemble
========================================


[STEP 1/2] Running comprehensive model comparison...
This will take 45-90 minutes depending on your system.
Models being trained:
  - Random Forest (tuned)
  - XGBoost (tuned)
  - LightGBM (tuned, if available)
  - CatBoost (tuned, if available)
  - GAM (tuned)
  - Neural Network (tuned)
  - Ensemble Meta-Learner (combines all)

── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.6
✔ forcats   1.0.1     ✔ stringr   1.6.0
✔ ggplot2   4.0.1     ✔ tibble    3.3.0
✔ lubridate 1.9.4     ✔ tidyr     1.3.1
✔ purrr     1.2.0     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
here() starts at /Users/phktistakis/Devoloper Projects/Markos project
Note: xgboost installation may require special setup
Note: lightgbm may need manual installation. Using alternative if unavailable.
Note: catboost may need manual installation. Using alternative if unavailable.
randomForest 4.7-1.2
Type rfNews() to see new features/changes/bug fixes.

Attaching package: ‘randomForest’

The following object is masked from ‘package:dplyr’:

    combine

The following object is masked from ‘package:ggplot2’:

    margin

Loading required package: lattice

Attaching package: ‘caret’

The following object is masked from ‘package:purrr’:

    lift

Loaded gbm 2.2.2
This version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3
Loading required package: nlme

Attaching package: ‘nlme’

The following object is masked from ‘package:dplyr’:

    collapse

This is mgcv 1.9-4. For overview type '?mgcv'.

Attaching package: ‘nnet’

The following object is masked from ‘package:mgcv’:

    multinom

Warning: xgboost not available, will use GBM for XGBoost-like model

=== STEP 1: Data Preparation ===
Loaded 265603 PHEV records
Numeric dataset: 245325 records, 44 predictors
Train: 196261 Test: 49064 

=== STEP 2: Random Forest - Full Tuning ===
Tuning Random Forest...
  New best: mtry= 3 ntree= 500 nodesize= 1 R²= 0.8706 
  New best: mtry= 3 ntree= 500 nodesize= 5 R²= 0.8714 
  New best: mtry= 3 ntree= 1000 nodesize= 1 R²= 0.8745 
  New best: mtry= 5 ntree= 500 nodesize= 1 R²= 0.8925 
  New best: mtry= 5 ntree= 500 nodesize= 5 R²= 0.8937 
  New best: mtry= 5 ntree= 1000 nodesize= 1 R²= 0.8948 
  New best: mtry= 5 ntree= 1000 nodesize= 5 R²= 0.8954 
  New best: mtry= 7 ntree= 500 nodesize= 1 R²= 0.9026 
  New best: mtry= 7 ntree= 500 nodesize= 5 R²= 0.9026 
  New best: mtry= 7 ntree= 1000 nodesize= 1 R²= 0.9041 
  New best: mtry= 10 ntree= 500 nodesize= 1 R²= 0.9083 
  New best: mtry= 10 ntree= 500 nodesize= 5 R²= 0.9086 
  New best: mtry= 10 ntree= 1000 nodesize= 1 R²= 0.91 
  New best: mtry= 10 ntree= 1000 nodesize= 5 R²= 0.9103 
Best RF params: mtry=10, ntree=1000, nodesize=5 
RF - Train R²: 0.9388 Test R²: 0.9188 Test Adj R²: 0.9187 Test RMSE: 1.6607 Test MAE: 1.0715 Test MAPE: 5.08 %

=== STEP 3: XGBoost - Full Tuning ===
Using GBM as XGBoost alternative...
Tuning XGBoost (GBM)...
  New best: n.trees= 500 depth= 4 shrinkage= 0.001 minobs= 5 R²= 0.3936 
  New best: n.trees= 500 depth= 4 shrinkage= 0.01 minobs= 5 R²= 0.8264 
  New best: n.trees= 500 depth= 4 shrinkage= 0.01 minobs= 10 R²= 0.8266 
  New best: n.trees= 500 depth= 4 shrinkage= 0.05 minobs= 5 R²= 0.9053 
  New best: n.trees= 500 depth= 6 shrinkage= 0.05 minobs= 5 R²= 0.919 
  New best: n.trees= 500 depth= 8 shrinkage= 0.05 minobs= 5 R²= 0.9279 
  New best: n.trees= 1000 depth= 6 shrinkage= 0.05 minobs= 5 R²= 0.9359 
  New best: n.trees= 1000 depth= 8 shrinkage= 0.05 minobs= 5 R²= 0.9429 
  New best: n.trees= 1500 depth= 6 shrinkage= 0.05 minobs= 5 R²= 0.9437 
  New best: n.trees= 1500 depth= 8 shrinkage= 0.05 minobs= 5 R²= 0.951 
Best XGBoost params: n.trees=1500, interaction.depth=8, shrinkage=0.05, n.minobsinnode=5 
XGBoost (GBM) - Train R²: 0.9345 Test R²: 0.9279 Test RMSE: 1.5647 

=== STEP 4: LightGBM - Full Tuning ===
LightGBM not available - skipping. Install with: install.packages('lightgbm') or from GitHub

=== STEP 5: CatBoost - Full Tuning ===
CatBoost not available - skipping. Install from: https://github.com/catboost/catboost

=== STEP 6: GAM - Full Tuning ===
Tuning GAM...
  New best: k= 8 R²= 0.6078 
  New best: k= 10 R²= 0.614 
  New best: k= 12 R²= 0.626 
GAM - Train R²: 0.6253 Test R²: 0.6205 Test RMSE: 3.5895 

=== STEP 7: Neural Network ===
Tuning Neural Network...
  New best: size= 5 decay= 0.001 R²= 0.8896 
  New best: size= 10 decay= 0.001 R²= 0.9098 
  New best: size= 10 decay= 0.1 R²= 0.9116 
  New best: size= 20 decay= 0.001 R²= 0.9135 
  New best: size= 20 decay= 0.01 R²= 0.921 
Best NN params: size=20, decay=0.01 
Neural Network - Train R²: 0.9233 Test R²: 0.9208 Test RMSE: 1.6401 

=== STEP 8: Ensemble with Meta-Learner (All Models) ===
Generating base model predictions for meta-learner...
Error: vector memory limit of 16.0 Gb reached, see mem.maxVSize()
Execution halted
