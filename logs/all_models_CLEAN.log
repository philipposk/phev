── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.6
✔ forcats   1.0.1     ✔ stringr   1.6.0
✔ ggplot2   4.0.1     ✔ tibble    3.3.0
✔ lubridate 1.9.4     ✔ tidyr     1.3.1
✔ purrr     1.2.0     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
here() starts at /Users/phktistakis/Devoloper Projects/Markos project
Loading required package: lattice

Attaching package: ‘caret’

The following object is masked from ‘package:purrr’:

    lift

randomForest 4.7-1.2
Type rfNews() to see new features/changes/bug fixes.

Attaching package: ‘randomForest’

The following object is masked from ‘package:dplyr’:

    combine

The following object is masked from ‘package:ggplot2’:

    margin

Loaded gbm 2.2.2
This version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3
Loading required package: nlme

Attaching package: ‘nlme’

The following object is masked from ‘package:dplyr’:

    collapse

This is mgcv 1.9-4. For overview type '?mgcv'.

Attaching package: ‘nnet’

The following object is masked from ‘package:mgcv’:

    multinom

Note: lightgbm not available - will skip LightGBM model
Note: catboost not available - will skip CatBoost model

========================================
ALL MODELS WITH CLEAN VARIABLES (26 vars)
Using best parameters from 44-variable runs
========================================


=== STEP 1: Data Preparation (CLEAN - No Circular Variables) ===
Loaded 265603 PHEV records
Excluding 25 circular variables
Using 27 clean variables
Clean dataset: 241771 records, 27 predictors
Train: 193419 Test: 48352 

Loading existing results from previous run...
Found existing results for: random_forest, xgb, gam, neural_network 
========================================
NOTE: Using BEST PARAMETERS from 44-variable runs
(No re-tuning to save time - parameters should generalize)
========================================


=== STEP 2: Random Forest (Clean Variables, Best Params) ===
Random Forest already completed. Skipping. Test R² = 0.7163 

=== STEP 3: XGBoost/GBM (Clean Variables, Best Params) ===
XGBoost/GBM already completed. Skipping. Test R² = 0.7371 

=== STEP 4: GAM (Clean Variables, Best Params) ===
Available variables for GAM: 27 
Fitting GAM with formula...
Error in gam_(region_numeric, k = 5, bs = "re") : 
  could not find function "gam_"
Calls: predict
Execution halted
── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.6
✔ forcats   1.0.1     ✔ stringr   1.6.0
✔ ggplot2   4.0.1     ✔ tibble    3.3.0
✔ lubridate 1.9.4     ✔ tidyr     1.3.1
✔ purrr     1.2.0     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
here() starts at /Users/phktistakis/Devoloper Projects/Markos project
Loading required package: lattice

Attaching package: ‘caret’

The following object is masked from ‘package:purrr’:

    lift

randomForest 4.7-1.2
Type rfNews() to see new features/changes/bug fixes.

Attaching package: ‘randomForest’

The following object is masked from ‘package:dplyr’:

    combine

The following object is masked from ‘package:ggplot2’:

    margin

Loaded gbm 2.2.2
This version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3
Loading required package: nlme

Attaching package: ‘nlme’

The following object is masked from ‘package:dplyr’:

    collapse

This is mgcv 1.9-4. For overview type '?mgcv'.

Attaching package: ‘nnet’

The following object is masked from ‘package:mgcv’:

    multinom

Note: lightgbm not available - will skip LightGBM model
Note: catboost not available - will skip CatBoost model

========================================
ALL MODELS WITH CLEAN VARIABLES (26 vars)
Using best parameters from 44-variable runs
========================================


=== STEP 1: Data Preparation (CLEAN - No Circular Variables) ===
Loaded 265603 PHEV records
Excluding 25 circular variables
Using 27 clean variables
Clean dataset: 241771 records, 27 predictors
Train: 193419 Test: 48352 

Loading existing results from previous run...
Found existing results for: random_forest, xgb, gam, neural_network 
========================================
NOTE: Using BEST PARAMETERS from 44-variable runs
(No re-tuning to save time - parameters should generalize)
========================================


=== STEP 2: Random Forest (Clean Variables, Best Params) ===
Random Forest already completed. Skipping. Test R² = 0.7163 

=== STEP 3: XGBoost/GBM (Clean Variables, Best Params) ===
XGBoost/GBM already completed. Skipping. Test R² = 0.7371 

=== STEP 4: GAM (Clean Variables, Best Params) ===
GAM already completed. Skipping. Test R² = 0.6431 

=== STEP 5: Neural Network (Clean Variables, Best Params) ===
Neural Network already completed. Skipping. Test R² = 0.7322 

=== STEP 6: LightGBM (Clean Variables, Best Params) ===
LightGBM not available - skipping. Install with: install.packages('lightgbm') or from GitHub

=== STEP 7: CatBoost (Clean Variables, Best Params) ===
CatBoost not available - skipping. Install from: https://github.com/catboost/catboost

=== STEP 8: Ensemble Meta-Learner (All Available Models) ===
Creating ensemble from available models...
Ensemble (Weighted Average) - Test R²: 0.7354 Test Adj R²: 0.7353 Test RMSE: 2.9779 Test MAE: 2.0688 Test MAPE: 9.56 %

=== Saving Results ===
Error: object 'rf_r2_train' not found
Execution halted
── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.6
✔ forcats   1.0.1     ✔ stringr   1.6.0
✔ ggplot2   4.0.1     ✔ tibble    3.3.0
✔ lubridate 1.9.4     ✔ tidyr     1.3.1
✔ purrr     1.2.0     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
here() starts at /Users/phktistakis/Devoloper Projects/Markos project
Loading required package: lattice

Attaching package: ‘caret’

The following object is masked from ‘package:purrr’:

    lift

randomForest 4.7-1.2
Type rfNews() to see new features/changes/bug fixes.

Attaching package: ‘randomForest’

The following object is masked from ‘package:dplyr’:

    combine

The following object is masked from ‘package:ggplot2’:

    margin

Loaded gbm 2.2.2
This version of gbm is no longer under development. Consider transitioning to gbm3, https://github.com/gbm-developers/gbm3
Loading required package: nlme

Attaching package: ‘nlme’

The following object is masked from ‘package:dplyr’:

    collapse

This is mgcv 1.9-4. For overview type '?mgcv'.

Attaching package: ‘nnet’

The following object is masked from ‘package:mgcv’:

    multinom

Note: lightgbm not available - will skip LightGBM model
Note: catboost not available - will skip CatBoost model

========================================
ALL MODELS WITH CLEAN VARIABLES (26 vars)
Using best parameters from 44-variable runs
========================================


=== STEP 1: Data Preparation (CLEAN - No Circular Variables) ===
Loaded 265603 PHEV records
Excluding 25 circular variables
Using 27 clean variables
Clean dataset: 241771 records, 27 predictors
Train: 193419 Test: 48352 

Loading existing results from previous run...
Found existing results for: random_forest, xgb, gam, neural_network 
========================================
NOTE: Using BEST PARAMETERS from 44-variable runs
(No re-tuning to save time - parameters should generalize)
========================================


=== STEP 2: Random Forest (Clean Variables, Best Params) ===
Random Forest already completed. Skipping. Test R² = 0.7163 

=== STEP 3: XGBoost/GBM (Clean Variables, Best Params) ===
XGBoost/GBM already completed. Skipping. Test R² = 0.7371 

=== STEP 4: GAM (Clean Variables, Best Params) ===
GAM already completed. Skipping. Test R² = 0.6431 

=== STEP 5: Neural Network (Clean Variables, Best Params) ===
Neural Network already completed. Skipping. Test R² = 0.7322 

=== STEP 6: LightGBM (Clean Variables, Best Params) ===
LightGBM not available - skipping. Install with: install.packages('lightgbm') or from GitHub

=== STEP 7: CatBoost (Clean Variables, Best Params) ===
CatBoost not available - skipping. Install from: https://github.com/catboost/catboost

=== STEP 8: Ensemble Meta-Learner (All Available Models) ===
Creating ensemble from available models...
Ensemble (Weighted Average) - Test R²: 0.7354 Test Adj R²: 0.7353 Test RMSE: 2.9779 Test MAE: 2.0688 Test MAPE: 9.56 %

=== Saving Results ===
Results saved to: tables/model_comparison_CLEAN_variables.csv
Models saved to: data/processed/all_models_CLEAN_variables.RData

========================================
ALL MODELS WITH CLEAN VARIABLES COMPLETE
========================================

Summary:
           Model                    Variables   Train_R2   Test_R2 Test_Adj_R2
1  Random Forest 26 clean (excludes circular)  0.7726774 0.7162716   0.7161131
2    XGBoost/GBM 26 clean (excludes circular)  0.7483612 0.7370855   0.7369386
3            GAM 26 clean (excludes circular) -0.2469435 0.6430577   0.6429986
4 Neural Network 26 clean (excludes circular)  0.7205732 0.7322184   0.7320688
5       Ensemble 26 clean (excludes circular)         NA 0.7354486   0.7353008
  Test_RMSE Test_MAE Test_MAPE
1  3.083907 2.130209  9.709346
2  2.968638 2.044082  9.369810
3  3.458987 2.475347 11.784269
4  2.995989 2.086671  9.573907
5  2.977865 2.068822  9.563724
                                                      Params_Source
1                 44-variable run (mtry=10, ntree=1000, nodesize=5)
2 44-variable run (n.trees=1500, depth=8, shrinkage=0.05, minobs=5)
3                                            44-variable run (k=12)
4                             44-variable run (size=20, decay=0.01)
5                                    Weighted average of all models

NOTE: These models use best parameters from 44-variable runs.
This approach is documented in the methodology section.
